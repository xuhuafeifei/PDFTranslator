 

\begin{center}
\includegraphics[width=333pt, height=151pt]{page_11_1.png}
\end{center}
\begin{center} Figure 11: Timeline for a configuration change. Dashed lines show configuration entries that have been created but not committed, and solid lines show the latest committed configuration entry. The leader first creates the $C_{\text {old,new }}$ configuration entry in its log and commits it to $C_{\text {old,new }}$ (a majority of $C_{\text {old }}$ and a majority of $C_{\text {new }}$ ). Then it creates the $C_{\text {new }}$ entry and commits it to a majority of $C_{\text {new }}$. There is no point in time in which $C_{\text {old }}$ and $C_{\text {new }}$ can both make decisions independently.

 \end{center} - Any server from either configuration may serve as leader.

- Agreement (for elections and entry commitment) requires separate majorities from both the old and new configurations.

The joint consensus allows individual servers to transition between configurations at different times without compromising safety. Furthermore, joint consensus allows the cluster to continue servicing client requests throughout the configuration change.

Cluster configurations are stored and communicated using special entries in the replicated log; Figure 11 illustrates the configuration change process. When the leader receives a request to change the configuration from $C_{\text {old }}$ to $C_{\text {new }}$, it stores the configuration for joint consensus ( $C_{\text {old,new }}$ in the figure) as a log entry and replicates that entry using the mechanisms described previously. Once a given server adds the new configuration entry to its log, it uses that configuration for all future decisions (a server always uses the latest configuration in its log, regardless of whether the entry is committed). This means that the leader will use the rules of $C_{\text {old,new }}$ to determine when the $\log$ entry for $C_{\text {old,new }}$ is committed. If the leader crashes, a new leader may be chosen under either $C_{\text {old }}$ or $C_{\text {old,new }}$, depending on whether the winning candidate has received $C_{\text {old,new }}$. In any case, $C_{\text {new }}$ cannot make unilateral decisions during this period.

Once $C_{\text {old,new }}$ has been committed, neither $C_{\text {old }}$ nor $C_{\text {new }}$ can make decisions without approval of the other, and the Leader Completeness Property ensures that only servers with the $C_{\text {old,new }}$ log entry can be elected as leader. It is now safe for the leader to create a log entry describing $C_{\text {new }}$ and replicate it to the cluster. Again, this configuration will take effect on each server as soon as it is seen. When the new configuration has been committed under the rules of $C_{\text {new }}$, the old configuration is irrelevant and servers not in the new configuration can be shut down. As shown in Figure 11, there is no time when $C_{\text {old }}$ and $C_{\text {new }}$ can both make unilateral decisions; this guarantees safety.

There are three more issues to address for reconfiguration. The first issue is that new servers may not initially store any log entries. If they are added to the cluster in this state, it could take quite a while for them to catch up, during which time it might not be possible to commit new log entries. In order to avoid availability gaps, Raft introduces an additional phase before the configuration change, in which the new servers join the cluster as non-voting members (the leader replicates log entries to them, but they are not considered for majorities). Once the new servers have caught up with the rest of the cluster, the reconfiguration can proceed as described above.

The second issue is that the cluster leader may not be part of the new configuration. In this case, the leader steps down (returns to follower state) once it has committed the $C_{\text {new }}$ log entry. This means that there will be a period of time (while it is committing $C_{\text {new }}$ ) when the leader is managing a cluster that does not include itself; it replicates log entries but does not count itself in majorities. The leader transition occurs when $C_{\text {new }}$ is committed because this is the first point when the new configuration can operate independently (it will always be possible to choose a leader from $C_{\text {new }}$ ). Before this point, it may be the case that only a server from $C_{\text {old }}$ can be elected leader.

The third issue is that removed servers (those not in $C_{\text {new }}$ ) can disrupt the cluster. These servers will not receive heartbeats, so they will time out and start new elections. They will then send RequestVote RPCs with new term numbers, and this will cause the current leader to revert to follower state. A new leader will eventually be elected, but the removed servers will time out again and the process will repeat, resulting in poor availability.

To prevent this problem, servers disregard RequestVote RPCs when they believe a current leader exists. Specifically, if a server receives a RequestVote RPC within the minimum election timeout of hearing from a current leader, it does not update its term or grant its vote. This does not affect normal elections, where each server waits at least a minimum election timeout before starting an election. However, it helps avoid disruptions from removed servers: if a leader is able to get heartbeats to its cluster, then it will not be deposed by larger term numbers.

\section*{7 Log compaction}

Raft's log grows during normal operation to incorporate more client requests, but in a practical system, it cannot grow without bound. As the log grows longer, it occupies more space and takes more time to replay. This will eventually cause availability problems without some mechanism to discard obsolete information that has accumulated in the log.

Snapshotting is the simplest approach to compaction. In snapshotting, the entire current system state is written to a snapshot on stable storage, then the entire log up to

