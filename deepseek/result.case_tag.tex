% !TEX program = xelatex
\documentclass[12pt,a4paper]{report} % 也可以改成 report 或 book

% ----------------------
% 中文支持
% ----------------------
\usepackage[UTF8]{ctex}          % 中文支持
\setCJKmainfont{Songti SC}
\setCJKsansfont{Heiti SC}
\setCJKmonofont{STFangsong}
\setmainfont{Times New Roman}     % 英文字体
\setsansfont{Arial}               % 英文无衬线
\setmonofont{Courier New}         % 英文等宽

% ----------------------
% 数学和符号
% ----------------------
\usepackage{amsmath, amssymb, amsfonts} 
\usepackage{geometry}            % 页面边距
\geometry{left=3cm, right=3cm, top=2.5cm, bottom=2.5cm}

% ----------------------
% 其他常用宏包
% ----------------------
\usepackage{graphicx}            % 图片
\usepackage{hyperref}            % 超链接
\usepackage{caption}             % 图表标题

% ----------------------
% 文档开始
% ----------------------
\begin{document}

评估提示 在 DeepSeek-V3 的设置下，使用 simpleevals 框架的提示对 MMLU、DROP、GPQA Diamond 和 SimpleQA 等标准基准进行评估。对于 MMLU-Redux，我们在零样本设置下采用 Zero-Eval 提示格式（Lin, 2024）。对于 MMLU-Pro、C-Eval 和 CLUE-WSC，由于原始提示是少样本提示，我们略微修改提示以适应零样本设置。少样本中的思维链（CoT）可能会影响 DeepSeek-R1 的性能。其他数据集则遵循其原始评估协议，并使用其创建者提供的默认提示。对于代码和数学基准，HumanEval-Mul 数据集涵盖了八种主流编程语言（Python、Java、C++、C\#{}、JavaScript、TypeScript、PHP 和 Bash）。模型在 LiveCodeBench 上的性能评估采用思维链（CoT）格式，数据收集时间为 2024 年 8 月至 2025 年 1 月。Codeforces 数据集通过使用 10 场 Div. 2 比赛的问题以及专家设计的测试用例进行评估，随后计算出预期的排名和参赛者的百分比。SWE-Bench 的验证结果是通过无代理框架（Xia et al., 2024）获得的。与 AIDER 相关的基准采用“diff”格式进行测量。DeepSeek-R1 在每个基准上的输出被限制在最多 32,768 个标记。

% ----------------------
% 文档结束
% ----------------------
\end{document}